{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing various modules to use\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import re\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "User given URL: https://www.kriyadocs.com/\n"
     ]
    }
   ],
   "source": [
    "# input user given url. it can be homepage or innerpage url\n",
    "url = input()\n",
    "print(\"User given URL: \"+url)"
   ]
  },
  {
   "source": [
    "Feature 1:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "https://www.kriyadocs.com/\n"
     ]
    }
   ],
   "source": [
    "# if user entered innerpage url then parse the domain page url\n",
    "parsed_uri = urlparse(url)\n",
    "homepage = '{uri.scheme}://{uri.netloc}/'.format(uri=parsed_uri)\n",
    "print(homepage)"
   ]
  },
  {
   "source": [
    "Feature 2:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defing the header for request because some websites doesn't allow web crawling(like https://www.garyvaynerchuk.com)\n",
    "hdr = ({'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'})\n",
    "req = get(homepage, headers=hdr)\n",
    "# using beautifulsoup to parse the webpage content\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    "result = [homepage.rstrip('/')]\n",
    "# defining regex to store only valid urls\n",
    "regex = re.compile(\n",
    "        r'^(?:http|ftp)s?://'  # http:// or https://\n",
    "        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|'  # domain...\n",
    "        r'localhost|'  # localhost...\n",
    "        r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})'  # ...or ip\n",
    "        r'(?::\\d+)?'  # optional port\n",
    "        r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n",
    "# if the webpage is an image webpage then we have to ommit that page\n",
    "image_url = \"\"\"\\.jpg\\Z|\\.jpeg\\Z|\\.jpe\\Z|\\.jif\\Z|\\.jfif\\Z|\\.jfi\\Z|\n",
    "            \\.png\\Z|\\.gif\\Z|\\.webp\\Z|\\.tiff\\Z|\\.tif\\Z|\\.psd\\Z|\n",
    "            \\.raw\\Z|\\.arw\\Z|\\.cr2\\Z|\\.nrw\\Z|\\.k25\\Z|\\.bmp\\Z|\\.dib\\Z|\n",
    "            \\.heif\\Z|\\.heic\\Z|\\.ind\\Z|\\.indd\\Z|\\.indt\\Z|\\.jp2\\Z|\n",
    "            \\.j2k\\Z|\\.jpf\\Z|\\.jpx\\Z|\\.jpm\\Z|\\.mj2\\Z|\\.svg\\Z|\\.svgz\\Z|\n",
    "            \\.ai\\Z|\\.eps\\Z|\\.pdf\\Z\"\"\"\n",
    "# finding all the valid urls and storing them into result list\n",
    "for link in soup.findAll('a'):\n",
    "    temp_url = urljoin(homepage, link.get('href')).rstrip('/')\n",
    "    if re.match(regex, temp_url) is not None:\n",
    "        if re.search(image_url, temp_url) is None:\n",
    "            result.append(temp_url)"
   ]
  },
  {
   "source": [
    "Feature 3:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['https://www.kriyadocs.com/#collapse-1-10',\n",
       " 'https://www.kriyadocs.com/our-story',\n",
       " 'https://www.kriyadocs.com/#ensure',\n",
       " 'https://www.facebook.com/kriyadocs',\n",
       " 'https://www.kriyadocs.com/blog',\n",
       " 'https://www.kriyadocs.com/#collapse-1-1',\n",
       " 'https://www.kriyadocs.com/#collapse-1-14',\n",
       " 'https://www.kriyadocs.com/#collapse-1-13',\n",
       " 'https://www.kriyadocs.com/#features',\n",
       " 'https://www.kriyadocs.com/terms-of-use',\n",
       " 'https://www.kriyadocs.com/#enhance',\n",
       " 'https://www.kriyadocs.com/#faq',\n",
       " 'https://www.kriyadocs.com/#collapse-1-2',\n",
       " 'https://www.kriyadocs.com/#benefits',\n",
       " 'https://www.kriyadocs.com/#enable',\n",
       " 'https://www.kriyadocs.com/publisherspeak',\n",
       " 'https://www.kriyadocs.com/#collapse-1-4',\n",
       " 'https://www.kriyadocs.com/#collapse-1-7',\n",
       " 'https://www.kriyadocs.com/#collapse-1-9',\n",
       " 'https://www.kriyadocs.com/#collapse-1-8',\n",
       " 'https://www.kriyadocs.com/#collapse-1-6',\n",
       " 'https://www.exeterpremedia.com',\n",
       " 'https://www.kriyadocs.com/#extend',\n",
       " 'https://www.kriyadocs.com/privacypolicy',\n",
       " 'https://www.linkedin.com/showcase/kriyadocs',\n",
       " 'https://www.kriyadocs.com',\n",
       " 'https://www.kriyadocs.com/#collapse-1-5',\n",
       " 'https://www.kriyadocs.com/#collapse-1-11',\n",
       " 'https://www.kriyadocs.com/#collapse-1-3',\n",
       " 'https://twitter.com/KriyaDocs']"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "# storing the valid and unique urls into homepage_urls list\n",
    "homepage_urls = list(set(result))\n",
    "homepage_urls"
   ]
  },
  {
   "source": [
    "Feature 4:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "https://www.kriyadocs.com/#collapse-1-10\n",
      "https://www.kriyadocs.com/our-story\n",
      "https://www.kriyadocs.com/#ensure\n",
      "https://www.facebook.com/kriyadocs\n",
      "https://www.kriyadocs.com/blog\n",
      "https://www.kriyadocs.com/#collapse-1-1\n",
      "https://www.kriyadocs.com/#collapse-1-14\n",
      "https://www.kriyadocs.com/#collapse-1-13\n",
      "https://www.kriyadocs.com/#features\n",
      "https://www.kriyadocs.com/terms-of-use\n",
      "https://www.kriyadocs.com/#enhance\n",
      "https://www.kriyadocs.com/#faq\n",
      "https://www.kriyadocs.com/#collapse-1-2\n",
      "https://www.kriyadocs.com/#benefits\n",
      "https://www.kriyadocs.com/#enable\n",
      "https://www.kriyadocs.com/publisherspeak\n",
      "https://www.kriyadocs.com/#collapse-1-4\n",
      "https://www.kriyadocs.com/#collapse-1-7\n",
      "https://www.kriyadocs.com/#collapse-1-9\n",
      "https://www.kriyadocs.com/#collapse-1-8\n",
      "https://www.kriyadocs.com/#collapse-1-6\n",
      "https://www.exeterpremedia.com\n",
      "https://www.kriyadocs.com/#extend\n",
      "https://www.kriyadocs.com/privacypolicy\n",
      "https://www.linkedin.com/showcase/kriyadocs\n",
      "https://www.kriyadocs.com\n",
      "https://www.kriyadocs.com/#collapse-1-5\n",
      "https://www.kriyadocs.com/#collapse-1-11\n",
      "https://www.kriyadocs.com/#collapse-1-3\n",
      "https://twitter.com/KriyaDocs\n"
     ]
    }
   ],
   "source": [
    "all_text = \"\"  # it containg the data of all the visited webpages\n",
    "url_titles = list()  # it contains the url titles of all visited webpages for later usage\n",
    "meta_data = {}  # it containg the meta data of webpages \n",
    "webpage_size = {}  # it contains the size of visited vebpages\n",
    "# comments contains all the comments present in webpage \n",
    "comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "for url in homepage_urls:\n",
    "    print(url)\n",
    "    try:\n",
    "        # opening each top level webpages\n",
    "        html_page = get(url, headers=hdr)\n",
    "        soup = BeautifulSoup(html_page.text, 'html.parser')\n",
    "        # storing the webpage size\n",
    "        webpage_size[url] = len(html_page.text)\n",
    "        if(soup.title is not None):\n",
    "            try:   \n",
    "                # storing the webpage title\n",
    "                url_titles.append(soup.title.string)\n",
    "                text = soup.find_all(text=True)\n",
    "                output = ''\n",
    "                # storing the content of the webpage decoposing the script, style and comments present in webpage\n",
    "                for t in text:\n",
    "                    if t.parent.name not in ['script', 'style']:\n",
    "                        if t not in comments:\n",
    "                            output += '{} '.format(t)\n",
    "                all_text += output\n",
    "                # storing the meta data of the webpage in dataframe format\n",
    "                meta_data_of_url = pd.DataFrame(columns=[\"name\", \"content\", \"http-equiv\", \"charset\"])\n",
    "                meta_tag = soup.findAll('meta')\n",
    "                for x in meta_tag:\n",
    "                    dic = {\"name\": \"\", \"content\": \"\", \"http-equiv\": \"\", \"charset\": \"\"}\n",
    "                    for key, value in x.attrs.items():\n",
    "                        if key in dic.keys():\n",
    "                            dic[key] = value\n",
    "                    meta_data_of_url = meta_data_of_url.append(dic, ignore_index=True)\n",
    "                meta_data[url] = meta_data_of_url\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = (line.strip() for line in all_text.splitlines())\n",
    "# break multi-headlines into a line each\n",
    "chunks = (phrase.strip() for line in lines for phrase in line.split())\n",
    "# drop blank lines\n",
    "all_text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "# removing the punctuations present in the data\n",
    "all_text = re.sub(r'[^\\w\\s]', '', all_text)\n",
    "all_text = (all_text.lower()).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the stopwords present in webpages because stopwords like is, the, a, an, etc are of no use in analysis of bigrams and trigrams\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(all_text)\n",
    "all_text = [w for w in word_tokens if not w in stop_words]\n",
    "all_text = \" \".join(all_text)"
   ]
  },
  {
   "source": [
    "Feature 5:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Bi-Grams:  ['html home', 'home kriyadocs', 'kriyadocs benefits', 'benefits features', 'features publisherspeak', 'publisherspeak blog', 'blog contact', 'contact faq', 'faq schedule', 'schedule demo']\nTri-Grams:  ['html home kriyadocs', 'home kriyadocs benefits', 'kriyadocs benefits features', 'benefits features publisherspeak', 'features publisherspeak blog', 'publisherspeak blog contact', 'blog contact faq', 'contact faq schedule', 'faq schedule demo', 'schedule demo accelerate']\n"
     ]
    }
   ],
   "source": [
    "# generating bi-grams from the stored content\n",
    "def getNGrams(n_gram):\n",
    "    temp_gram = list()\n",
    "    temp = all_text.split()\n",
    "    if(n_gram == \"bi_gram\"):\n",
    "        t1 = 1\n",
    "        t2 = 2\n",
    "    if(n_gram == \"tri_gram\"):\n",
    "        t1 = 2\n",
    "        t2 = 3\n",
    "    for x in range(0, len(temp)-t1):\n",
    "        st = \"\"\n",
    "        for y in range(x, x+t2):\n",
    "            st = st+temp[y]+\" \"\n",
    "        st = st.strip()\n",
    "        temp_gram.append(st)\n",
    "    return temp_gram\n",
    "\n",
    "bi_gram = getNGrams(\"bi_gram\")\n",
    "tri_gram = getNGrams(\"tri_gram\")\n",
    "print(\"Bi-Grams: \",bi_gram[0:10])\n",
    "print(\"Tri-Grams: \",tri_gram[0:10])"
   ]
  },
  {
   "source": [
    "Outputs:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.txt', 'w', encoding=\"utf-8\") as file:\n",
    "    # writing number of top level pages present in homepage\n",
    "    file.write(\"The number of top level pages are \"+str(len(homepage_urls)))\n",
    "    # writing top level page titles in file\n",
    "    file.write(\"\\n\\n\\n<----- Page Titles of Top level Pages ----->\")\n",
    "    all_urls = set(url_titles)\n",
    "    x = 1\n",
    "    for i in all_urls:\n",
    "        file.write(\"\\n%d %s\" % (x, i.strip()))\n",
    "        x += 1\n",
    "    \"\"\"\n",
    "    file.write(\"\\n\\n\\n<----- Meta Data of Each top table page ----->\")\n",
    "\n",
    "    for key, value in meta_data.items():\n",
    "        file.write(\"\\n\\nURL:->%s\\n\"%key)\n",
    "        file.write(str(value.values))\n",
    "    \"\"\"\n",
    "# writing metadata content in metadata.csv file\n",
    "for key, value in meta_data.items():\n",
    "    with open('metadata.csv', 'a', encoding=\"utf-8\") as file:\n",
    "        file.write(\"\\n\\n\"+str(key)+\"\\n\\n\")\n",
    "    value.to_csv('metadata.csv', mode=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_links = list()  # storing internal links \n",
    "external_links = list()  # storing external links\n",
    "for url in homepage_urls:\n",
    "    parsed_uri = urlparse(url)\n",
    "    domain = '{uri.scheme}://{uri.netloc}/'.format(uri=parsed_uri)\n",
    "    if domain == homepage:\n",
    "        internal_links.append(url)\n",
    "    else:\n",
    "        external_links.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.txt', 'a', encoding=\"utf-8\") as file:\n",
    "    # writing internal links in the file\n",
    "    file.write(\"\\n\\n\\n<----- Internal Links ----->\")\n",
    "    x = 1\n",
    "    for url in internal_links:\n",
    "        file.write(\"\\n%d %s\" % (x, url.strip()))\n",
    "        x += 1\n",
    "    # writing external links in the file\n",
    "    file.write(\"\\n\\n\\n<----- External Links ----->\")\n",
    "    x = 1\n",
    "    for url in external_links:\n",
    "        file.write(\"\\n%d %s\" % (x, url.strip()))\n",
    "        x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get top 20 words of the bi-gram and tri-gram generated\n",
    "def getTopWords(n_gram):\n",
    "    dic_gram = {}\n",
    "    for i in n_gram:\n",
    "        if i in dic_gram.keys():\n",
    "            dic_gram[i] = dic_gram.get(i)+1\n",
    "        else:\n",
    "            dic_gram[i] = 1\n",
    "    counts = {k: v for k, v in sorted(dic_gram.items(), key=lambda x: x[1], reverse=True)}\n",
    "    top_grams = {}\n",
    "    count = 0\n",
    "    for key, value in counts.items():\n",
    "        if count == 20:\n",
    "            break\n",
    "        else:\n",
    "            top_grams[key] = value\n",
    "        count = count+1\n",
    "    return top_grams\n",
    "\n",
    "\n",
    "# calling and storing top 20 words of bigrams and trigrams each\n",
    "top_bigrams = getTopWords(bi_gram)\n",
    "top_trigrams = getTopWords(tri_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.txt', 'a', encoding=\"utf-8\") as file:\n",
    "    # writing top 20 bigrams in file\n",
    "    file.write(\"\\n\\n\\n<----- Top 20 Bi-Grams ----->\")\n",
    "    x = 1\n",
    "    for key, value in top_bigrams.items():\n",
    "        file.write(\"\\n%d) %s %d\" % (x, key, value))\n",
    "        x += 1\n",
    "    # writing top 20 trigrams in file\n",
    "    file.write(\"\\n\\n\\n<----- Top 20 Tri-Grams ----->\")\n",
    "    x = 1\n",
    "    for key, value in top_trigrams.items():\n",
    "        file.write(\"\\n%d) %s %d\" % (x, key, value))\n",
    "        x += 1\n",
    "    # writing webpage size of each top level pages\n",
    "    file.write(\"\\n\\n\\n<----- Webpage Size ----->\")\n",
    "    x = 1\n",
    "    for key, value in webpage_size.items():\n",
    "        file.write(\"\\n%d) %s \\t ..... \\t %d\" % (x, key, value))\n",
    "        x += 1\n",
    "    # writing min webpage size\n",
    "    file.write(\"\\n\\n\\n<----- Minimum Webpage Size ----->\")\n",
    "    file.write(\"\\n%d\" % min(list(webpage_size.values())))\n",
    "    # writing max webpage size\n",
    "    file.write(\"\\n\\n\\n<----- Maximum Webpage Size ----->\")\n",
    "    file.write(\"\\n%d\" % max(list(webpage_size.values())))\n",
    "    # writing average webpage size\n",
    "    file.write(\"\\n\\n\\n<----- Average Webpage Size ----->\")\n",
    "    file.write(\"\\n%d\" % (sum(list(webpage_size.values()))/len(webpage_size.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}